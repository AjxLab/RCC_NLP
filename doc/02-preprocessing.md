# 前処理

このチャプターでは，自然言語処理におけるテキストの前処理について解説します。


## なぜ必要なのか
処理対象のテキストは一般に非構造データです。

特にWebテキストにはHTMLタグやJavaScriptのコードといった処理する際にノイズとなる情報が含まれています。このようなノイズは，前処理によって取り除かなければ期待する結果は得られません。


## 前処理の種類
本チャプターでは，下記に示す6つの前処理について解説していきます。

1. テキストのクリーニング
2. 単語分割
3. 単語の正規化
4. ストップワードの除去
5. 単語のID化
6. パディング


### 1. テキストのクリーニング
テキストのクリーニングとは，不要な文字を除去する処理のことです。この処理には，**Webスクレイピング**や**正規表現**（**Regular Expression**）などがよく用いられます。

> 正規表現とは、文字列の集合を一つの文字列で表現する方法の一つである。正則表現とも呼ばれ、形式言語理論の分野では比較的こちらの訳語の方が使われる。

ここでは，正規表現を扱ったクリーニングのみ扱います。

Pythonで正規表現を扱うには，組み込みモジュールのreを使用します。例えば，下記のテキストからハッシュタグ（#Python）を除去してみましょう。

```python
>>> text = "Pythonでテキストをクリーニングする。 #Python"
```

まず，ハッシュタグを一般化します。

* 「#」からはじまる文字列
* 「#」以降は，アルファベット（大文字・小文字）が１文字以上続く

上記で立てた仮説から，ハッシュタグを除去できる正規表現パターンを考えます。

re.subメソッドは，指定した正規表現パターンにマッチする箇所を置換する処理を行います。第１引数には正規表現パターン，第２引数には置換後の文字列，第３引数には対象の文字列を指定します。

```python
>>> import re
>>> def clean_hashtag(text):
...     cleaned_text = re.sub(r'#[a-zA-Z]+', '', text)
...     return cleaned_text
```

ここで定義した関数に先ほど宣言したテキストを代入すると。。

```python
>>> clean_hashtag(text)
'Pythonでテキストをクリーニングする。 '
```

このように，ハッシュタグを除去することに成功しました。


## 2. 単語分割
形態素解析器としてJanomeを利用します。

```python
>>> from janome.tokenizer import Tokenizer
>>> text = "私は昨日の夕食を覚えていない。"
>>> t = Tokenizer()
>>> for token in t.tokenize(text):
...     print(token)
```

このように入力すると，下記の結果が得られます。

```
私      名詞,代名詞,一般,*,*,*,私,ワタシ,ワタシ
は      助詞,係助詞,*,*,*,*,は,ハ,ワ
昨日    名詞,副詞可能,*,*,*,*,昨日,キノウ,キノー
の      助詞,連体化,*,*,*,*,の,ノ,ノ
夕食    名詞,一般,*,*,*,*,夕食,ユウショク,ユーショク
を      助詞,格助詞,一般,*,*,*,を,ヲ,ヲ
覚え    動詞,自立,*,*,一段,連用形,覚える,オボエ,オボエ
て      助詞,接続助詞,*,*,*,*,て,テ,テ
い      動詞,非自立,*,*,一段,未然形,いる,イ,イ
ない    助動詞,*,*,*,特殊・ナイ,基本形,ない,ナイ,ナイ
。      記号,句点,*,*,*,*,。,。,。
```

上の例では品詞や読みなどの情報も得られましたが，多くの場合では単語分割で十分です。単語分割のみを行いたい場合には，Tokenizerに'''wakati=True'''を指定することで，単語のリストが得られます。

```python
>>> t = Tokenizer(wakati=True)
>>> t.tokenize(text)
['私', 'は', '昨日', 'の', '夕食', 'を', '覚え', 'て', 'い', 'ない', '。']
```
