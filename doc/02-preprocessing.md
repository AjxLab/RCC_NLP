# 前処理

このチャプターでは，自然言語処理におけるテキストの前処理について解説します。


## なぜ必要なのか
処理対象のテキストは一般に非構造データです。

特にWebテキストにはHTMLタグやJavaScriptのコードといった処理する際にノイズとなる情報が含まれています。このようなノイズは，前処理によって取り除かなければ期待する結果は得られません。


## 前処理の種類
本チャプターでは，下記に示す6つの前処理について解説していきます。

1. テキストのクリーニング
2. 単語分割
3. 単語の正規化
4. ストップワードの除去


### 1. テキストのクリーニング
テキストのクリーニングとは，不要な文字を除去する処理のことです。この処理には，**Webスクレイピング**や**正規表現**（**Regular Expression**）などがよく用いられます。
（[Pythonでやさしくしっかり学ぶ正規表現](https://qiita.com/simonritchie/items/43b3dfd9419f442d48ad)）

> 正規表現とは、文字列の集合を一つの文字列で表現する方法の一つである。正則表現とも呼ばれ、形式言語理論の分野では比較的こちらの訳語の方が使われる。

ここでは，正規表現を扱ったクリーニングのみ扱います。

Pythonで正規表現を扱うには，組み込みモジュールのreを使用します。例えば，下記のテキストからハッシュタグ（#Python）を除去してみましょう。

```python
>>> text = "Pythonでテキストをクリーニングする。 #Python"
```

まず，ハッシュタグを一般化します。

* 「#」からはじまる文字列
* 「#」以降は，アルファベット（大文字・小文字）が１文字以上続く

上記で立てた仮説から，ハッシュタグを除去できる正規表現パターンを考えます。

re.subメソッドは，指定した正規表現パターンにマッチする箇所を置換する処理を行います。第１引数には正規表現パターン，第２引数には置換後の文字列，第３引数には対象の文字列を指定します。

```python
>>> import re
>>> def clean_hashtag(text):
...     cleaned_text = re.sub(r'#[a-zA-Z]+', '', text)
...     return cleaned_text
```

ここで定義した関数に先ほど宣言したテキストを代入すると。。

```python
>>> clean_hashtag(text)
'Pythonでテキストをクリーニングする。 '
```

このように，ハッシュタグを除去することに成功しました。


### 2. 単語分割
形態素解析器としてJanomeを利用します。

```python
>>> from janome.tokenizer import Tokenizer
>>> text = "私は昨日の夕食を覚えていない。"
>>> t = Tokenizer()
>>> for token in t.tokenize(text):
...     print(token)
```

このように入力すると，下記の結果が得られます。

```
私      名詞,代名詞,一般,*,*,*,私,ワタシ,ワタシ
は      助詞,係助詞,*,*,*,*,は,ハ,ワ
昨日    名詞,副詞可能,*,*,*,*,昨日,キノウ,キノー
の      助詞,連体化,*,*,*,*,の,ノ,ノ
夕食    名詞,一般,*,*,*,*,夕食,ユウショク,ユーショク
を      助詞,格助詞,一般,*,*,*,を,ヲ,ヲ
覚え    動詞,自立,*,*,一段,連用形,覚える,オボエ,オボエ
て      助詞,接続助詞,*,*,*,*,て,テ,テ
い      動詞,非自立,*,*,一段,未然形,いる,イ,イ
ない    助動詞,*,*,*,特殊・ナイ,基本形,ない,ナイ,ナイ
。      記号,句点,*,*,*,*,。,。,。
```

上の例では品詞や読みなどの情報も得られましたが，多くの場合では単語分割で十分です。単語分割のみを行いたい場合には，Tokenizerに'''wakati=True'''を指定することで，単語のリストが得られます。

```python
>>> t = Tokenizer(wakati=True)
>>> t.tokenize(text)
['私', 'は', '昨日', 'の', '夕食', 'を', '覚え', 'て', 'い', 'ない', '。']
```


### 3. 単語の正規化
単語の正規化では，単語の文字手の統一，つづりや表記揺れの吸収といった単語を置換する処理を行います。ここでは，下記の３処理について解説していきます。

* 文字書の統一
* 数字の置き換え
* 辞書を用いた単語の統一

#### 文字種の統一
文字種の統一タスクには，
* 大文字・小文字の統一
* 半角カタカナを全角カタカナに変換
などがあります。

![](../img/02/convert.png)

```python
>>> text = "Ritsumeikan Computer Club is a great organization."
>>> text.lower()  # 小文字化
'ritsumeikan computer club is a great organization.'

>>> text.upper()  # 大文字化
'RITSUMEIKAN COMPUTER CLUB IS A GREAT ORGANIZATION.'
```

#### 数値を置換
自然言語処理において，数値はさほど重要な役割を持たないことが多いです。
不必要なタスクにおいては，数値を全て同一の文字に置換をすることで，ノイズと見なして除去をします。

```python
>>> import re
>>> def normalize_number(text):
...     replaced_text = re.sub(r'\d+', '0', text)
...     return replaced_text
...
>>> text = '私の誕生日は2005年05月05日です。'
>>> normalize_number(text)
'私の誕生日は0年0月0日です。'
```


### 4. ストップワードの除去
ストップワードとは，一般的に多用されるなどの理由で処理対象外とする単語のことです。

例えば，助詞や助動詞などの機能語（「は」「の」「です」「ます」...）が挙げられます。

ここでは，日本語ストップワード辞書として有名な[Slothlib](http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt)を例に除去をしていきます。

```text
あそこ
あたり
あちら
あっち
あと
あな
あなた
あれ
いくつ
いつ
いま
いや
いろいろ
...
```

まずは，[Slothlib](http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt)の中身を`stopwords.txt`にコピーして下さい。

改行区切りのストップワード辞書を作成しました。それではこれをPythonから読み込み，前処理対象のテキストに含まれていた場合にはそれを除去していきましょう。

```python
>>> text = '結局あそこのラーメンが一番美味しい。'
>>>
>>> stopwords = open('stopwords.txt', 'r').read().split('\n')
>>> stopwords
['あそこ', 'あたり', 'あちら', 'あっち', ... , '同じ', '感じ']
>>>
>>> for sw in stopwords:
...     text = text.replace(sw, '')
...
>>> text
'のラーメンが番美味しい。'
```
