# 前処理

このチャプターでは，自然言語処理におけるテキストの前処理について解説します。


## なぜ必要なのか
処理対象のテキストは一般に非構造データです。

特にWebテキストにはHTMLタグやJavaScriptのコードといった処理する際にノイズとなる情報が含まれています。このようなノイズは，前処理によって取り除かなければ期待する結果は得られません。


## 前処理の種類
本チャプターでは，下記に示す4つの前処理について解説していきます。

1. テキストのクリーニング
2. 単語分割
3. 単語の正規化
4. ストップワードの除去


### 1. テキストのクリーニング
テキストのクリーニングとは，不要な文字を除去する処理のことです。この処理には，**Webスクレイピング**や**正規表現**（**Regular Expression**）などがよく用いられます。
（[Pythonでやさしくしっかり学ぶ正規表現](https://qiita.com/simonritchie/items/43b3dfd9419f442d48ad)）

> 正規表現とは、文字列の集合を一つの文字列で表現する方法の一つである。正則表現とも呼ばれ、形式言語理論の分野では比較的こちらの訳語の方が使われる。

ここでは，正規表現を扱ったクリーニングのみ扱います。

Pythonで正規表現を扱うには，組み込みモジュールのreを使用します。例えば，下記のテキストからハッシュタグ（#Python）を除去してみましょう。

```python
>>> text = "Pythonでテキストをクリーニングする。 #Python"
```

まず，ハッシュタグを一般化します。

* 「#」からはじまる文字列
* 「#」以降は，アルファベット（大文字・小文字）が１文字以上続く

上記で立てた仮説から，ハッシュタグを除去できる正規表現パターンを考えます。

re.subメソッドは，指定した正規表現パターンにマッチする箇所を置換する処理を行います。第１引数には正規表現パターン，第２引数には置換後の文字列，第３引数には対象の文字列を指定します。

```python
>>> import re
>>> def clean_hashtag(text):
...     cleaned_text = re.sub(r'#[a-zA-Z]+', '', text)
...     return cleaned_text
```

ここで定義した関数に先ほど宣言したテキストを代入すると。。

```python
>>> clean_hashtag(text)
'Pythonでテキストをクリーニングする。 '
```

このように，ハッシュタグを除去することに成功しました。


### 2. 単語分割
形態素解析器としてJanomeを利用します。

```python
>>> from janome.tokenizer import Tokenizer
>>> text = "私は昨日の夕食を覚えていない。"
>>> t = Tokenizer()
>>> for token in t.tokenize(text):
...     print(token)
```

このように入力すると，下記の結果が得られます。

```
私      名詞,代名詞,一般,*,*,*,私,ワタシ,ワタシ
は      助詞,係助詞,*,*,*,*,は,ハ,ワ
昨日    名詞,副詞可能,*,*,*,*,昨日,キノウ,キノー
の      助詞,連体化,*,*,*,*,の,ノ,ノ
夕食    名詞,一般,*,*,*,*,夕食,ユウショク,ユーショク
を      助詞,格助詞,一般,*,*,*,を,ヲ,ヲ
覚え    動詞,自立,*,*,一段,連用形,覚える,オボエ,オボエ
て      助詞,接続助詞,*,*,*,*,て,テ,テ
い      動詞,非自立,*,*,一段,未然形,いる,イ,イ
ない    助動詞,*,*,*,特殊・ナイ,基本形,ない,ナイ,ナイ
。      記号,句点,*,*,*,*,。,。,。
```

上の例では品詞や読みなどの情報も得られましたが，多くの場合では単語分割で十分です。単語分割のみを行いたい場合には，Tokenizerに'''wakati=True'''を指定することで，単語のリストが得られます。

```python
>>> t = Tokenizer(wakati=True)
>>> t.tokenize(text)
['私', 'は', '昨日', 'の', '夕食', 'を', '覚え', 'て', 'い', 'ない', '。']
```

前述では形態素解析を用いた単語分割について解説しましたが，次はN-gramを用いた手法を紹介します。
> 任意の文字列や文書を連続したn個の文字で分割するテキスト分割方法．特に，nが1の場合をユニグラム（uni-gram），2の場合をバイグラム（bi-gram），3の場合をトライグラム（tri-gram）と呼ぶ．

要するにテキストの分割方法の一種なのですが，形態素解析が単語レベルで分割するのに対し，N-gramでは文字レベルで分割を行います。

```python
>>> def n_gram(target, n):
...     return [target[idx:idx + n] for idx in range(len(target) - n + 1)]
...
>>> n_gram("私は昨日の夕食を覚えていない。", 1)  # uni-gram
['私', 'は', '昨', '日', 'の', '夕', '食', 'を', '覚', 'え', 'て', 'い', 'な', 'い', '。']
>>> n_gram("私は昨日の夕食を覚えていない。", 2)  # bi-gram
['私は', 'は昨', '昨日', '日の', 'の夕', '夕食', '食を', 'を覚', '覚え', 'えて', 'てい', 'いな', 'ない', 'い。']
>>> n_gram("私は昨日の夕食を覚えていない。", 3)  # tri-gram
['私は昨', 'は昨日', '昨日の', '日の夕', 'の夕食', '夕食を', '食を覚', 'を覚え', '覚えて', 'えてい', 'ていな', 'いない', 'ない。']
```

N-gramを利用したテキスト分割のメリットは，**辞書の精度に左右されない分割が可能である**という点です。

より詳しく知りたい方は，`N-gram 形態素解析 使い分け`などで検索をしてみてはいかがでしょうか。


### 3. 単語の正規化
単語の正規化では，単語の文字手の統一，つづりや表記揺れの吸収といった単語を置換する処理を行います。ここでは，下記の３処理について解説していきます。

* 文字書の統一
* 数字の置き換え
* 辞書を用いた単語の統一

#### 文字種の統一
文字種の統一タスクには，
* 大文字・小文字の統一
* 半角カタカナを全角カタカナに変換
などがあります。

![](../img/02/convert.png)

```python
>>> text = "Ritsumeikan Computer Club is a great organization."
>>> text.lower()  # 小文字化
'ritsumeikan computer club is a great organization.'

>>> text.upper()  # 大文字化
'RITSUMEIKAN COMPUTER CLUB IS A GREAT ORGANIZATION.'
```

#### 数値を置換
自然言語処理において，数値はさほど重要な役割を持たないことが多いです。
不必要なタスクにおいては，数値を全て同一の文字に置換をすることで，ノイズと見なして除去をします。

```python
>>> import re
>>> def normalize_number(text):
...     replaced_text = re.sub(r'\d+', '0', text)
...     return replaced_text
...
>>> text = '私の誕生日は2005年05月05日です。'
>>> normalize_number(text)
'私の誕生日は0年0月0日です。'
```


### 4. ストップワードの除去
ストップワードとは，一般的に多用されるなどの理由で処理対象外とする単語のことです。

例えば，助詞や助動詞などの機能語（「は」「の」「です」「ます」...）が挙げられます。

ここでは，日本語ストップワード辞書として有名な[Slothlib](http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt)を例に除去をしていきます。

```text
あそこ
あたり
あちら
あっち
あと
あな
あなた
あれ
いくつ
いつ
いま
いや
いろいろ
...
```

まずは，[Slothlib](http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt)の中身を`stopwords.txt`にコピーして下さい。

改行区切りのストップワード辞書を作成しました。それではこれをPythonから読み込み，前処理対象のテキストに含まれていた場合にはそれを除去していきましょう。

```python
>>> text = '結局あそこのラーメンが一番美味しい。'
>>>
>>> stopwords = open('stopwords.txt', 'r').read().split('\n')
>>> stopwords
['あそこ', 'あたり', 'あちら', 'あっち', ... , '同じ', '感じ']
>>>
>>> for sw in stopwords:
...     text = text.replace(sw, '')
...
>>> text
'のラーメンが番美味しい。'
```

辞書が不十分であるためまだまだ荒いですが，単語の出現頻度などからストップワードを割り出すことで，より高性能なストップワード除去を実現することができます。

今度は，こう頻度の単語をストップワードとみなす例を紹介します。ここでは出現頻度を分析する対象のコーパスとして[ja.text8](https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/ja.text8.zip)を利用します。

```python
>>> words = open('ja.text8', 'r').read().split()
```

続いて読み込んだ単語の出現頻度をカウントしてみましょう。collectionsのCounterクラスを使うと簡単に出現頻度をカウントでき，most_common(n)メソッドを使って出現頻度上位n件を取り出すことができます。

```python
>>> from collections import Counter
>>> counter = Counter(words)
>>> counter.most_common(10)
[('の', 828585), ('、', 785716), ('。', 532921), ('に', 527014), ('は', 488009), ('を', 423115), ('た', 421908), ('が', 353221), ('で', 350821), ('て', 259995)]
```

基本的には高頻度の単語をストップワードとしますが，時には低頻度のものを指定することもあります。nの値を色々と変えて試してみてください。
